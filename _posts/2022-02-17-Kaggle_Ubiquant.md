---
layout: post
section-type: post
title: 캐글 Ubiquant Market Prediction 데이터 분석 대회
category: kaggle
tags: [ 'kaggle','deep learning','machine learning','data science','Ubiquant' ]
---

안녕하세요, Pythonash 입니다.

오늘은 캐글에서 진행되는 Ubiquant Market Prediction 대회를 참가한 후기를 포스팅 하려 합니다.

어제 오늘해서 한 12시간? 정도 작업을 했는데 일단 0.149 score 까지 올려봤는데요, 

1등이 현재 0.157인데 저 정도 까진 아니어도 Top 10에 들려면 꽤 많이 작업해봐야 할 것 같아요...

안그래도 할게 많은 대학원생인데 ㅠㅠㅠㅠ 세상엔 고수가 정말 많은 것 같습니다...

그래도 나름 이정도면 중상위권이니 일단은 만족하고 있습니다 ㅋㅋㅋㅋ

대회에 대한 간략한 설명을 드리자면,

# Ubiquant Market Prediction 대회란?

## 1. 목표

아시다시피, 금융시장엔 수 많은 변동성이 존재합니다.

이전 부터 많은 계량경제학자, 데이터 과학자, 펀드 매니저 등 각 분야의 전문가들이 그 변동성을 예측하기 위해 노력해 왔는데요.

이번 대회의 목표는 투자 수익률을 자신만의 적절한 모델로 예측하는 것 입니다.

금융 시장의 데이터다 보니 딥러닝을 활용할 때 시계열 모델(RNN, LSTM, and GRU 등)을 활용하는게 좋아보일 것 같지만

시계열로 이루어진 데이터를 예측하는 것은 아니고 평활 데이터형태로 이루어진 데이터의 값을 예측하는 것이라 저는 심층 신경망(DNN) 모델을 사용했습니다.

평가지표는 각 데이터 포인트들 끼리의 피어슨 상관계수입니다.

그리고 결과물이 노트북으로 이루어지는 것인데, 특정한 API를 이용해서 제출하는 형식이라 캐글이나 파이썬에 익숙하지 않으신 분들은 어려우실 수도 있을 것 같습니다.

## 2. 문제

이전에 리뷰한 타이타닉 데이터셋과는 다르게 이번 데이터셋에는 결측값은 없습니다.

하지만, 데이터양이 정말 방대한데 대략 19 GB정도 됩니다. 이미지가 아닌 숫자로만요.

따라서 더 낮은 메모리로 정제된 데이터셋을 불러와 작업하는 것이 좋습니다.

데이터 용량이 큰 만큼 변수의 수도 300개가 넘는 수 인데, 분석도 분석이지만 노트북을 실행하는 동안 메모리가 터져버리는 경우가 많아서 메모리 핸들링이 꼭 필요한 대회 입니다.

더군다나, 변수의 수가 많아서 EDA도 쉽지는 않은데 결론적으로는 성능을 높이기 위해서 **모든 변수를 사용하는 것**이 가장 좋긴 합니다.

물론 저의 접근 방법이 꼭 정답도 아니고, 분석 방법은 데이터를 다루는 사람에 따라 천차만별이라 각기 다를 수는 있습니다만

그래도 저의 노트북이 처음 대회를 접하고자 하시는 분들에게 도움이 될 수 있도록 대회에 참가해보고 그 노트북을 공유해보려고 합니다.

## 3. 코드

코드는 파이썬으로 작성 했고, 제 <span><a href="https://github.com/Pythonash/Kaggle-Kor-">깃허브</a></span> 에서 보실 수 있습니다.

## 4. 결과

<img width="967" alt="스크린샷 2022-02-17 오후 5 37 43" src="https://user-images.githubusercontent.com/91790368/154437290-8007c331-4f39-430d-8044-d9d7dc9ef126.png">

당연한 얘기일 수 있지만, 고작 0.008차이임에도 1등과 저 사이에 저만큼의 참가자들이 있습니다 ㅋㅋㅋ...

시간이 나면 한번 랭크를 더 올려보도록 하겠습니다.

궁금하신 점 댓글이나 이메일로 알려주시면 최대한 답변 드리겠습니다.

감사합니다.
궁금하신 점 댓글이나 이메일로 알려주시면 최대한 답변 드리겠습니다.
